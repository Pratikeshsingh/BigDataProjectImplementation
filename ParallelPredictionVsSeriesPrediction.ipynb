{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ParallelPredictionVsSeriesPrediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ6yzM16nx0D",
        "colab_type": "code",
        "outputId": "3eb8200a-1860-4d13-e11d-3ef4bc7b2b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "from multiprocessing import Pool, Process\n",
        "import numpy as np\n",
        "import psutil\n",
        "import scipy.signal\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "num_trials = 2\n",
        "\n",
        "# Count the number of physical CPUs.\n",
        "num_cpus = psutil.cpu_count(logical=False)\n",
        "print('Using {} cores.'.format(num_cpus))\n",
        "\n",
        "################################################\n",
        "###### Benchmark 1: numerical computation ######\n",
        "################################################\n",
        "\n",
        "\n",
        "def f(args):\n",
        "    image, random_filter = args\n",
        "    # Do some image processing.\n",
        "    return scipy.signal.convolve2d(image, random_filter)[::5, ::5]\n",
        "\n",
        "\n",
        "pool = Pool(num_cpus)\n",
        "\n",
        "filters = [np.random.normal(size=(4, 4)) for _ in range(num_cpus)]\n",
        "\n",
        "\n",
        "def run_benchmark():\n",
        "    image = np.zeros((3000, 3000))\n",
        "    pool.map(f, zip(num_cpus * [image], filters))\n",
        "\n",
        "\n",
        "durations1 = []\n",
        "for _ in range(num_trials):\n",
        "    start_time = time.time()\n",
        "\n",
        "    run_benchmark()\n",
        "\n",
        "    duration1 = time.time() - start_time\n",
        "    durations1.append(duration1)\n",
        "    print('Numerical computation workload took {} seconds.'.format(duration1))\n",
        "\n",
        "###############################################\n",
        "###### Benchmark 2: stateful computation ######\n",
        "###############################################\n",
        "\n",
        "\n",
        "def accumulate_prefixes(args):\n",
        "    running_prefix_count, running_popular_prefixes, document = args\n",
        "    for word in document:\n",
        "        for i in range(1, len(word)):\n",
        "            prefix = word[:i]\n",
        "            running_prefix_count[prefix] += 1\n",
        "            if running_prefix_count[prefix] > 3:\n",
        "                running_popular_prefixes.add(prefix)\n",
        "    return running_prefix_count, running_popular_prefixes\n",
        "\n",
        "\n",
        "pool = Pool(num_cpus)\n",
        "\n",
        "durations2 = []\n",
        "for _ in range(num_trials):\n",
        "    running_prefix_counts = [defaultdict(int) for _ in range(4)]\n",
        "    running_popular_prefixes = [set() for _ in range(4)]\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i in range(10):\n",
        "        documents = [[np.random.bytes(20) for _ in range(10000)]\n",
        "                     for _ in range(num_cpus)]\n",
        "        results = pool.map(\n",
        "            accumulate_prefixes,\n",
        "            zip(running_prefix_counts, running_popular_prefixes, documents))\n",
        "        running_prefix_counts = [result[0] for result in results]\n",
        "        running_popular_prefixes = [result[1] for result in results]\n",
        "\n",
        "    popular_prefixes = set()\n",
        "    for prefixes in running_popular_prefixes:\n",
        "        popular_prefixes |= prefixes\n",
        "\n",
        "    duration2 = time.time() - start_time\n",
        "    durations2.append(duration2)\n",
        "    print('Stateful computation workload took {} seconds.'.format(duration2))\n",
        "\n",
        "###################################################\n",
        "###### Benchmark 3: expensive initialization ######\n",
        "###################################################\n",
        "\n",
        "\n",
        "def save_model():\n",
        "    mnist = tf.keras.datasets.mnist.load_data()\n",
        "    x_train, y_train = mnist[0]\n",
        "    x_train = x_train / 255.0\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    # Train the model.\n",
        "    model.fit(x_train, y_train, epochs=1)\n",
        "    # Save the model to disk.\n",
        "    filename = '/tmp/model'\n",
        "    model.save(filename)\n",
        "\n",
        "\n",
        "# Train and save the model. This has to be done in a separate process because\n",
        "# otherwise Python multiprocessing will hang when you try do run the code\n",
        "# below.\n",
        "p = Process(target=save_model)\n",
        "p.start()\n",
        "p.join()\n",
        "\n",
        "filename = '/tmp/model'\n",
        "\n",
        "\n",
        "def evaluate_next_batch(i):\n",
        "    # Pin the process to a specific core if we are on Linux to prevent\n",
        "    # contention between the different processes since TensorFlow uses\n",
        "    # multiple threads.\n",
        "    if sys.platform == 'linux':\n",
        "        psutil.Process().cpu_affinity([i])\n",
        "    model = tf.keras.models.load_model(filename)\n",
        "    mnist = tf.keras.datasets.mnist.load_data()\n",
        "    x_test = mnist[1][0] / 255.0\n",
        "    return model.predict(x_test)\n",
        "\n",
        "\n",
        "pool = Pool(num_cpus)\n",
        "\n",
        "durations3 = []\n",
        "for _ in range(num_trials):\n",
        "    start_time = time.time()\n",
        "\n",
        "    for _ in range(10):\n",
        "        pool.map(evaluate_next_batch, range(num_cpus))\n",
        "\n",
        "    duration3 = time.time() - start_time\n",
        "    durations3.append(duration3)\n",
        "    print('Expensive initialization workload took {} seconds.'.format(duration3))\n",
        "\n",
        "print('Used {} cores.'.format(num_cpus))\n",
        "\n",
        "print(\"\"\"\n",
        "Results:\n",
        "- Numerical computation: {} +/- {}\n",
        "- Stateful computation: {} +/- {}\n",
        "- Expensive initialization: {} +/- {}\n",
        "\"\"\".format(np.mean(durations1), np.std(durations1),\n",
        "           np.mean(durations2), np.std(durations2),\n",
        "           np.mean(durations3), np.std(durations3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 1 cores.\n",
            "Numerical computation workload took 1.125704050064087 seconds.\n",
            "Numerical computation workload took 0.8151633739471436 seconds.\n",
            "Stateful computation workload took 13.845899820327759 seconds.\n",
            "Stateful computation workload took 13.56156849861145 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBz9L576n2HI",
        "colab_type": "code",
        "outputId": "ed9e4006-74c4-49fb-d6bf-6a1ad22675d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import psutil\n",
        "import scipy.signal\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "num_trials = 5\n",
        "\n",
        "# Count the number of physical CPUs.\n",
        "num_cpus = psutil.cpu_count(logical=False)\n",
        "print('Using {} cores.'.format(num_cpus))\n",
        "\n",
        "################################################\n",
        "###### Benchmark 1: numerical computation ######\n",
        "################################################\n",
        "\n",
        "\n",
        "def f(image, random_filter):\n",
        "    # Do some image processing.\n",
        "    return scipy.signal.convolve2d(image, random_filter)[::5, ::5]\n",
        "\n",
        "\n",
        "filters = [np.random.normal(size=(4, 4)) for _ in range(num_cpus)]\n",
        "\n",
        "\n",
        "def run_benchmark():\n",
        "    image = np.zeros((3000, 3000))\n",
        "    [f(image, filters[i]) for i in range(num_cpus)]\n",
        "\n",
        "\n",
        "durations1 = []\n",
        "for _ in range(num_trials):\n",
        "    start_time = time.time()\n",
        "\n",
        "    run_benchmark()\n",
        "\n",
        "    duration1 = time.time() - start_time\n",
        "    durations1.append(duration1)\n",
        "    print('Numerical computation workload took {} seconds.'.format(duration1))\n",
        "\n",
        "###############################################\n",
        "###### Benchmark 2: stateful computation ######\n",
        "###############################################\n",
        "\n",
        "\n",
        "class StreamingPrefixCount(object):\n",
        "    def __init__(self):\n",
        "        self.prefix_count = defaultdict(int)\n",
        "        self.popular_prefixes = set()\n",
        "\n",
        "    def add_document(self, document):\n",
        "        for word in document:\n",
        "            for i in range(1, len(word)):\n",
        "                prefix = word[:i]\n",
        "                self.prefix_count[prefix] += 1\n",
        "                if self.prefix_count[prefix] > 3:\n",
        "                    self.popular_prefixes.add(prefix)\n",
        "\n",
        "    def get_popular(self):\n",
        "        return self.popular_prefixes\n",
        "\n",
        "\n",
        "durations2 = []\n",
        "for _ in range(num_trials):\n",
        "    streaming_actors = [StreamingPrefixCount() for _ in range(num_cpus)]\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i in range(num_cpus * 10):\n",
        "        document = [np.random.bytes(20) for _ in range(10000)]\n",
        "        streaming_actors[i % num_cpus].add_document(document)\n",
        "\n",
        "    # Aggregate all of the results.\n",
        "    results = [actor.get_popular() for actor in streaming_actors]\n",
        "    popular_prefixes = set()\n",
        "    for prefixes in results:\n",
        "        popular_prefixes |= prefixes\n",
        "\n",
        "    duration2 = time.time() - start_time\n",
        "    durations2.append(duration2)\n",
        "    print('Stateful computation workload took {} seconds.'.format(duration2))\n",
        "\n",
        "###################################################\n",
        "###### Benchmark 3: expensive initialization ######\n",
        "###################################################\n",
        "\n",
        "mnist = tf.keras.datasets.mnist.load_data()\n",
        "x_train, y_train = mnist[0]\n",
        "x_train = x_train / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "# Train the model.\n",
        "model.fit(x_train, y_train, epochs=1)\n",
        "# Save the model to disk.\n",
        "filename = '/tmp/model'\n",
        "model.save(filename)\n",
        "\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self):\n",
        "        # Load the model and some data.\n",
        "        self.model = tf.keras.models.load_model(filename)\n",
        "        mnist = tf.keras.datasets.mnist.load_data()\n",
        "        self.x_test = mnist[1][0] / 255.0\n",
        "\n",
        "    def evaluate_next_batch(self):\n",
        "        # Note that we reuse the same data over and over, but in a\n",
        "        # real application, the data would be different each time.\n",
        "        return self.model.predict(self.x_test)\n",
        "\n",
        "\n",
        "actor = Model()\n",
        "\n",
        "durations3 = []\n",
        "for _ in range(num_trials):\n",
        "    start_time = time.time()\n",
        "\n",
        "    for j in range(10):\n",
        "        results = [actor.evaluate_next_batch() for _ in range(num_cpus)]\n",
        "\n",
        "    duration3 = time.time() - start_time\n",
        "    durations3.append(duration3)\n",
        "    print('Expensive initialization workload took {} seconds.'.format(duration3))\n",
        "\n",
        "print('Used {} cores.'.format(num_cpus))\n",
        "\n",
        "print(\"\"\"\n",
        "Results:\n",
        "- Numerical computation: {} +/- {}\n",
        "- Stateful computation: {} +/- {}\n",
        "- Expensive initialization: {} +/- {}\n",
        "\"\"\".format(np.mean(durations1), np.std(durations1),\n",
        "           np.mean(durations2), np.std(durations2),\n",
        "           np.mean(durations3), np.std(durations3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 1 cores.\n",
            "Numerical computation workload took 0.6866893768310547 seconds.\n",
            "Numerical computation workload took 0.6582543849945068 seconds.\n",
            "Numerical computation workload took 0.6543757915496826 seconds.\n",
            "Numerical computation workload took 0.6595430374145508 seconds.\n",
            "Numerical computation workload took 0.6546082496643066 seconds.\n",
            "Stateful computation workload took 2.800020217895508 seconds.\n",
            "Stateful computation workload took 2.8248794078826904 seconds.\n",
            "Stateful computation workload took 2.8060741424560547 seconds.\n",
            "Stateful computation workload took 2.7280027866363525 seconds.\n",
            "Stateful computation workload took 2.8515570163726807 seconds.\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2204 - accuracy: 0.9342\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: /tmp/model/assets\n",
            "Expensive initialization workload took 4.910231828689575 seconds.\n",
            "Expensive initialization workload took 4.859525442123413 seconds.\n",
            "Expensive initialization workload took 4.853824853897095 seconds.\n",
            "Expensive initialization workload took 4.797860383987427 seconds.\n",
            "Expensive initialization workload took 4.794317722320557 seconds.\n",
            "Used 1 cores.\n",
            "\n",
            "Results:\n",
            "- Numerical computation: 0.6626941680908203 +/- 0.012165433046538386\n",
            "- Stateful computation: 2.8021067142486573 +/- 0.04117138325752241\n",
            "- Expensive initialization: 4.843152046203613 +/- 0.043169016648038044\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tguC-AkWGWas",
        "colab_type": "text"
      },
      "source": [
        "Just the model part\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g9D9Y52o-E2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "from multiprocessing import Pool, Process\n",
        "import numpy as np\n",
        "import psutil\n",
        "import scipy.signal\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "num_trials = 5\n",
        "\n",
        "# Count the number of physical CPUs.\n",
        "num_cpus = psutil.cpu_count(logical=False)\n",
        "\n",
        "\n",
        "def save_model():\n",
        "    mnist = tf.keras.datasets.mnist.load_data()\n",
        "    x_train, y_train = mnist[0]\n",
        "    x_train = x_train / 255.0\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    # Train the model.\n",
        "    model.fit(x_train, y_train, epochs=1)\n",
        "    # Save the model to disk.\n",
        "    filename = '/tmp/model'\n",
        "    model.save(filename)\n",
        "\n",
        "\n",
        "# Train and save the model. This has to be done in a separate process because\n",
        "# otherwise Python multiprocessing will hang when you try do run the code\n",
        "# below.\n",
        "p = Process(target=save_model)\n",
        "p.start()\n",
        "p.join()\n",
        "\n",
        "filename = '/tmp/model'\n",
        "\n",
        "\n",
        "def evaluate_next_batch(i):\n",
        "    # Pin the process to a specific core if we are on Linux to prevent\n",
        "    # contention between the different processes since TensorFlow uses\n",
        "    # multiple threads.\n",
        "    if sys.platform == 'linux':\n",
        "        psutil.Process().cpu_affinity([i])\n",
        "    model = tf.keras.models.load_model(filename)\n",
        "    mnist = tf.keras.datasets.mnist.load_data()\n",
        "    x_test = mnist[1][0] / 255.0\n",
        "    return model.predict(x_test)\n",
        "\n",
        "\n",
        "pool = Pool(num_cpus)\n",
        "\n",
        "durations3 = []\n",
        "for _ in range(num_trials):\n",
        "    start_time = time.time()\n",
        "\n",
        "    for _ in range(10):\n",
        "        pool.map(evaluate_next_batch, range(num_cpus))\n",
        "\n",
        "    duration3 = time.time() - start_time\n",
        "    durations3.append(duration3)\n",
        "    print('Expensive initialization workload took {} seconds.'.format(duration3))\n",
        "\n",
        "print('Used {} cores.'.format(num_cpus))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axT2o4N6GdVG",
        "colab_type": "code",
        "outputId": "4a3097c4-7aff-4b76-d594-4b746701707c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sys.platform"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'linux'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoCGNskn0EOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from multiprocessing import Pool\n",
        "import psutil\n",
        "import sys\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8YxXhfT0Ji4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "num_cpus = psutil.cpu_count(logical=False)\n",
        "\n",
        "filename = '/tmp/model'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUMtKRe60LR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def evaluate_next_batch(i):\n",
        "    # Pin the process to a specific core if we are on Linux to prevent\n",
        "    # contention between the different processes since TensorFlow uses\n",
        "    # multiple threads.\n",
        "    if sys.platform == 'linux':\n",
        "        psutil.Process().cpu_affinity([i])\n",
        "    model = tf.keras.models.load_model(filename)\n",
        "    mnist = tf.keras.datasets.mnist.load_data()\n",
        "    x_test = mnist[1][0] / 255.0\n",
        "    return model.predict(x_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvniCGBY0M3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "pool = Pool(num_cpus)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RFsILGZ0Oeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for _ in range(10):\n",
        "    #pool.map(evaluate_next_batch, range(num_cpus))\n",
        "    print(_)\n",
        "    print(evaluate_next_batch)\n",
        "    print(num_cpus)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnf5QIXA1Lja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_next_batch(i):\n",
        "  if sys.platform == 'linux':\n",
        "    psutil.Process().cpu_affinity([i])\n",
        "  model=tf.keras.models.load_model(filename)\n",
        "  mnist = tf.keras.datasets.mnist.load_data()\n",
        "  x_test = mnist[1][0] / 255.0\n",
        "  model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSoWv0Sa0QOz",
        "colab_type": "code",
        "outputId": "850c31ff-31f3-4d99-cf51-ce21e1ac6079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(range(num_cpus))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "range(0, 1)\n",
            "range(0, 1)\n",
            "range(0, 1)\n",
            "range(0, 1)\n",
            "range(0, 1)\n",
            "range(0, 1)\n",
            "range(0, 1)\n",
            "range(0, 1)\n",
            "range(0, 1)\n",
            "range(0, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoBDiJZA1UHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjmM0fkq1WL2",
        "colab_type": "code",
        "outputId": "06e32d3e-4c2e-4268-c160-2ccd95b14f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.38016087e-07, 1.12755416e-07, 4.86328972e-05, ...,\n",
              "        9.99576747e-01, 2.68568033e-06, 4.34771209e-05],\n",
              "       [2.29172492e-05, 5.72527852e-03, 9.75866377e-01, ...,\n",
              "        2.09616982e-08, 1.92292660e-04, 5.00768760e-09],\n",
              "       [1.68155566e-05, 9.97583270e-01, 4.19404620e-04, ...,\n",
              "        1.36637106e-03, 1.76737245e-04, 1.80361058e-05],\n",
              "       ...,\n",
              "       [3.46943239e-08, 4.45942391e-07, 4.06168766e-07, ...,\n",
              "        5.70913762e-05, 1.68910576e-03, 6.25970133e-04],\n",
              "       [1.70961607e-06, 8.89005094e-07, 1.33031941e-08, ...,\n",
              "        3.23465287e-07, 5.03362331e-04, 4.81461804e-08],\n",
              "       [3.05578737e-06, 1.81200621e-09, 6.46509898e-06, ...,\n",
              "        3.89563604e-10, 1.62250387e-07, 1.32534881e-08]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ighYZAmF1YQI",
        "colab_type": "code",
        "outputId": "a288c2cd-a49c-48a9-dddc-79e544e5ae1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.mnist.load_data()\n",
        "x_train, y_train = mnist[0]\n",
        "x_train = x_train / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "# Train the model.\n",
        "model.fit(x_train, y_train, epochs=1)\n",
        "# Save the model to disk.\n",
        "filename = '/tmp/model'\n",
        "model.save(filename)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2176 - accuracy: 0.9353\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: /tmp/model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcyOxvNv2fV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}